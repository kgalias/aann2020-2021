{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the PyTorch tutorial [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (which we'll mostly be following this week) and Lilian Weng's [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq and attention\n",
    "Today we'll be teaching a neural network to translate from Polish to English. We'll be using a [sequence to sequence learning](https://arxiv.org/abs/1409.3215), in which two recurrent neural networks (and *encoder* and a *decoder*) work together to transform one sequence into another.\n",
    "![layer based](figures/seq2seq.png)\n",
    "\n",
    "<center>Source: <a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a>.</center>\n",
    "\n",
    "Later we'll also use an [attention mechanism](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) to improve upon our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need to download the data for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!wget https://www.manythings.org/anki/pol-eng.zip\n",
    "!unzip pol-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the class `Lang` to help us manage our data. Each word in a language will have its own separate index, as well as a count of how often it shows up (this will later help us replace rare words). Additionally we'll define three special indices:\n",
    "* 0 for the start-of-sequence token (SOS),\n",
    "* 1 for the end-of-sequence token (EOS),\n",
    "* 2 for padding (to make all batch sequences equal length so as to enable GPU parallelization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
    "        self.n_words = 3 # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = s.replace(\"ł\", \"l\")\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file you need to split the file into lines, and then split lines into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(f'{lang1}.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the problem we'll remove sentences which are above 20 words in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) <= MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "- read text file and split into lines, split lines into pairs,\n",
    "- normalize text, filter by length,\n",
    "- make word lists from sentences in pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('pol', 'eng', True)\n",
    "for _ in range(3):\n",
    "    print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need some additional functions to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "def pad_sequences(data_batch):\n",
    "    pl_batch, en_batch = [], []\n",
    "    for pl_sentence, en_sentence in data_batch:\n",
    "        pl_batch += [pl_sentence]\n",
    "        en_batch += [en_sentence]\n",
    "    pl_batch = pad_sequence(pl_batch, padding_value=PAD_token, batch_first=True)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_token, batch_first=True)\n",
    "    return pl_batch, en_batch\n",
    "\n",
    "\n",
    "def prepare_dataset(batch_size):\n",
    "    rng = np.random.RandomState(567)\n",
    "    indices = np.arange(len(pairs))\n",
    "    rng.shuffle(indices)\n",
    "    train_indices = indices[:int(len(pairs) * 0.8)]\n",
    "    test_indices = indices[int(len(pairs) * 0.8):]\n",
    "    train_pairs = list(pairs[idx] for idx in train_indices)\n",
    "    test_pairs = list(pairs[idx] for idx in test_indices)\n",
    "    tensor_train_pairs = [tensorsFromPair(pairs[idx]) for idx in train_indices]\n",
    "    tensor_test_pairs = [tensorsFromPair(pairs[idx]) for idx in test_indices]\n",
    "\n",
    "    train_loader = DataLoader(tensor_train_pairs, batch_size=batch_size, shuffle=True, collate_fn=pad_sequences)\n",
    "    test_loader = DataLoader(tensor_test_pairs, batch_size=batch_size, shuffle=True, collate_fn=pad_sequences)\n",
    "    return train_pairs, test_pairs, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "\n",
    "The encoder reads an input sequence and outputs a single vector, and the decoder reads that vector to produce an output sequence. Unlike sequence prediction with a single RNN, where every input corresponds to an output, the seq2seq model frees us from sequence length and order, which makes it ideal for translation between two languages.\n",
    "\n",
    "![layer based](figures/seq2seq_chainer.png)<center>Source: <a href=\"https://docs.chainer.org/en/stable/examples/seq2seq.html\">Write a Sequence to Sequence (seq2seq) Model</a>.</center>\n",
    "\n",
    "### Teacher forcing\n",
    "\n",
    "*Teacher forcing* is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. This can help us converge faster, but may also exhibit some instability. Because of PyTorch's autograd engine we'll be able to randomly choose whether to use teacher forcing or not with a simple if-statement (`teacher_forcing_ratio` will modify how much of it is present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some additional helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, inputs, targets=None, max_len=MAX_LENGTH):\n",
    "    batch_size = inputs.size(0)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(inputs)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]] * batch_size, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_output, decoder_attention = decoder(\n",
    "        decoder_input,\n",
    "        decoder_hidden,\n",
    "        targets=targets,\n",
    "        max_len=max_len,\n",
    "        encoder_outputs=encoder_outputs)\n",
    "    return decoder_output, decoder_attention\n",
    "\n",
    "def translate(encoder, decoder, sentence):\n",
    "    inputs = tensorFromSentence(input_lang, sentence).unsqueeze(0).to(device)\n",
    "    decoder_output, decoder_attention = predict(encoder, decoder, inputs)\n",
    "\n",
    "    decoded_words = []\n",
    "    for word in decoder_output[0]:\n",
    "        top_word = word.argmax(-1).item()\n",
    "        decoded_words.append(output_lang.index2word[top_word])\n",
    "        if top_word == EOS_token:\n",
    "            break\n",
    "\n",
    "    if decoder_attention is not None:\n",
    "        # [out_words, in_words]\n",
    "        att = decoder_attention.cpu().detach().numpy()\n",
    "        att = att[0, :len(decoded_words), :]\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.imshow(att)\n",
    "        ax.set_xticklabels([''] + sentence.split(' ') + ['EOS'], rotation=90)\n",
    "        ax.set_yticklabels([''] + decoded_words)\n",
    "\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return decoded_words\n",
    "        \n",
    "def translate_randomly(encoder, decoder, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = translate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, lr=0.01, batch_size=256, teacher_forcing_ratio=0.5, n_epochs=100):\n",
    "\n",
    "    train_pairs, test_pairs, train_loader, test_loader = prepare_dataset(batch_size)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs + 1):\n",
    "\n",
    "        epoch_train_loss = 0.\n",
    "        for in_batch, out_batch in train_loader:\n",
    "            in_batch, out_batch = in_batch.to(device), out_batch.to(device)\n",
    "\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "        \n",
    "            teacher_inputs = out_batch if random.random() < teacher_forcing_ratio else None\n",
    "        \n",
    "            decoder_output, decoded_attention = predict(\n",
    "                encoder, decoder, in_batch,\n",
    "                targets=teacher_inputs,\n",
    "                max_len=out_batch.size(1)\n",
    "            )\n",
    "        \n",
    "            loss = criterion(decoder_output.transpose(1, 2), out_batch)\n",
    "            loss.backward()\n",
    "        \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                print(\"=\" * 25, \"Translation test\", \"=\" * 25)\n",
    "                translate_randomly(encoder, decoder, test_pairs, n=5)\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / len(train_loader)\n",
    "        print(f\"Epoch: {epoch+1}. Train loss: {mean_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple seq2seq encoder, where we only use the last output (often called a *context vector*, as it encodes the context of the entire sentence). This context vector is used as the initial hidden state of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1p)\n",
    "Implement the decoder network for the seq2seq model. \n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string (SOS) token, and the first hidden state is the context vector (the encoder’s last hidden state). \n",
    "\n",
    "The decoder is supposed to return two variables:\n",
    "- `output`: a tensor of shape `(batch_size, seq_len, vocab_size)` representing the logits, which after applying a softmax (done outside of the decoder) will represent the probabilities of different words predicted by the decoder,\n",
    "- `attention_weights`: in this task always `None`.\n",
    "\n",
    "Some remarks:\n",
    "* Use `batch_first=True` when defining the RNN.\n",
    "* In the encoder we could call the LSTM class once, as we already had access to all of the words we wanted to translate. This is not the case for the decoder, as the input for time $t+1$ is the output for time $t$ (hence you'll probably need a for-loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        ???\n",
    "\n",
    "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
    "        if targets is not None:  # teacher forcing\n",
    "            ???\n",
    "        else:\n",
    "            ???\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train a model with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "embedding_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "\n",
    "train(encoder, decoder, lr=0.005, n_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "If only the context vector is passed between the encoder and decoder, that single vector carries the burden of encoding the entire sentence. Attention is a mechanism that allows the network to focus on a different part of the encoder's outputs for every step of the decoder's outputs. \n",
    "\n",
    "Rather than building a single context vector out of the encoder's last hidden state, the main idea of attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\n",
    "\n",
    "![](figures/An-attention-based-seq2seq-model.ppm)\n",
    "\n",
    "In the simpler decoder, at timestep $t$ the input was the embedded representation $\\mathbf{\\bar{y}_t}$. In a decoder with attention the input will be a concatenation of that vector and a vector $\\mathbf{z_t}$ created from the outputs of the encoder: $\\mathbf{\\tilde{h}_t} = [\\mathbf{\\bar{y}_t}, \\mathbf{z_t}]$. \n",
    "\n",
    "The vector $\\mathbf{z_t}$ is produced by the attention mechanism. Intuitively, we would like it to contain the information from the encoder which will be the most important for decoding a given word. \n",
    "Assume we have access to a *score function* $\\mathtt{score}(\\mathbf{h}, \\mathbf{e})$, which can tell us how similar the hidden state of the decoder $\\mathbf{h}$ and the word representation $\\mathbf{e}$ are.\n",
    "\n",
    "Then $w_i = \\frac{ \\exp(\\mathtt{score}(\\mathbf{h}, \\mathbf{e_i})) }{\\sum_{j} \\exp(\\mathtt{score}(\\mathbf{h}, \\mathbf{e_j}))}$ and \n",
    "$\\mathbf{z_t} = \\sum_i w_i \\cdot \\mathbf{e_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1p)\n",
    "Implement the decoder network for the seq2seq model utilizing the attention mechanism.\n",
    "\n",
    "The decoder:\n",
    "- receives an additional tensor `encoder_outputs` of shape `(batch_size, encoder_seq_len, hidden_size)` (these are the representations $\\mathbf{e_i}$, which you need to use in the attention mechanism).\n",
    "- outputs an additional tensor `attention_weights` of shape `(batch_size, decoder_seq_len, encoder_seq_len)` containing the weights $w_i$ (the values of this tensor should sum to one on the last dimension). \n",
    "\n",
    "The scoring function $\\mathtt{score}(\\mathbf{h}, \\mathbf{e})$ is going to be a neural network with two hidden layers of dimensions `hidden_size+hidden_size`, `hidden_size`, and `1`, respectively, with the `tanh` activation function after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        ???\n",
    "\n",
    "    def forward(self, input, hidden, targets=None, max_len=None, encoder_outputs=None):\n",
    "\n",
    "        ???\n",
    "\n",
    "        return output, seq_att_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train a model with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "embedding_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(output_lang.n_words, embedding_size, hidden_size).to(device)\n",
    "\n",
    "train(encoder, decoder, lr=0.005, n_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
