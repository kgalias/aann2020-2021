{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [A Gentle Introduction to `torch.autograd`](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) and the [video](https://www.youtube.com/watch?v=tIeHLnjs5U8) from 3Blue1Brown (the whole [playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) is worth a watch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation in PyTorch\n",
    "Training neural networks happens in two stages:\n",
    "- **Forward propagation** -- where the network makes its best guess about what the output should be. This is done by running the input data through the parametrized functions that constitute the network.\n",
    "- **Backward propagation** -- where the network updates its parameters in proportion to the error of its guess. This is done by traversing backward from the output, collecting the derivatives of the error with respect to the parameters of the functions, and optimizing the parameters using gradient descent. \n",
    "\n",
    "We can also remind ourselves of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) of calculus. If we have $L(x)=g(f(x))$, then:\n",
    "\n",
    "\n",
    "$$\\frac{d L(x)}{d x} = \\frac{d L(x)}{ d f(x)} \\frac{d f(x)}{d x}.$$\n",
    "\n",
    "In the context of automatic differentiation in PyTorch what's important is that to calculate the gradient $\\frac{d L(x)}{d x}$ we don't need to know anything about $g(x)$ if we know $\\frac{d L(x)}{ d f(x)}$. If each module in PyTorch can keep track of its own gradient and be fed in the gradient from the next layer, then calculating the gradient of even a complicated function is possible in this chain-like manner.\n",
    "\n",
    "In PyTorch each function that we use has these two methods:\n",
    "\n",
    "- `forward`, which gets fed $x$ and calculates $f(x)$.\n",
    "- `backward`, which gets fed $\\frac{d L(x)}{ d f(x)}$ and calculates $\\frac{d L(x)}{d x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: multiplication\n",
    "\n",
    "Let's say we want to implement a new multiplication function $f(x,y)=a\\cdot b.$ in PyTorch and we want this function to be able to calculate its own derivatives.\n",
    "\n",
    "To do that, we have to implement a `torch.autograd.Function` object with the methods:\n",
    "- `forward`:\n",
    "    1. Gets in `a` and `b`.\n",
    "    2. Saves them for later to help with calculating gradients.\n",
    "    3. Returns `a * b`.\n",
    "- `backward`:\n",
    "    1. Gets in `grad_output` (i.e. $\\frac{d L(x)}{d f(a,b)}$).\n",
    "    2. Retrieves `a` and `b` from memory.\n",
    "    3. Calculates the derivative $\\frac{d f(a,b)}{d a} = \\frac{d (a \\cdot b)}{d a}=b$.\n",
    "    4. Calculates the derivative $\\frac{d f(a,b)}{d b} = \\frac{d (a \\cdot b)}{d b}=a$.\n",
    "    5. Returns the derivatives $\\frac{d L(x)}{d f(a,b)}\\frac{d f(a,b)}{d a}$ and $\\frac{d L(x)}{d f(a,b)} \\frac{d f(a,b)}{d b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyProduct(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return a * b\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        # retrieve a and b from memory\n",
    "        a, b = self.saved_tensors\n",
    "        # calculate the derivative wrt a\n",
    "        a_grad = b\n",
    "        # calculate the derivative wrt b\n",
    "        b_grad = a\n",
    "        # return the derivatives\n",
    "        return grad_output * a_grad, grad_output * b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.autograd.gradcheck` checks whether the gradients calculated by our function match with numerical estimates from small finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn(20, 30, dtype=torch.double, requires_grad=True) * 2 - 5\n",
    "b = torch.randn(20, 30, dtype=torch.double, requires_grad=True) + 6\n",
    "\n",
    "prod_fn = MyProduct.apply\n",
    "assert torch.autograd.gradcheck(prod_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: ReLU\n",
    "$\\mathtt{ReLU}(x) = \\max(x,0)$ is currently the most popular activation function in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"$\\mathtt{ReLU}(x)$\")\n",
    "_ = plt.plot(np.linspace(-6, 6), np.maximum(np.linspace(-6, 6), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        x, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[x < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "relu_fn = MyReLU.apply\n",
    "assert torch.autograd.gradcheck(relu_fn, a, eps=1e-6, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1p)\n",
    "Implement the following functions:\n",
    "- `MyAdd(a, b): a + b`,\n",
    "- `MyDiv(a, b): a / b`,\n",
    "- `MySigmoid(x): 1 / (1 + exp(-x))`,\n",
    "- `ReQU(x): max(0, x*x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAdd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        return ???\n",
    "\n",
    "add_fn = MyAdd.apply\n",
    "assert torch.autograd.gradcheck(add_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDiv(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, a, b):\n",
    "        self.save_for_backward(a, b)\n",
    "        return ???\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        a, b = self.saved_tensors\n",
    "        return ???\n",
    "\n",
    "div_fn = MyDiv.apply\n",
    "assert torch.autograd.gradcheck(div_fn, (a, b), eps=1e-3, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySigmoid(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return ???\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        x, = self.saved_tensors\n",
    "        return ??? \n",
    "\n",
    "sigmoid_fn = MySigmoid.apply\n",
    "assert torch.autograd.gradcheck(sigmoid_fn, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReQU(torch.autograd.Function):\n",
    "    def forward(self, x):\n",
    "        self.save_for_backward(x)\n",
    "        return ???\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x, = self.saved_tensors\n",
    "        return ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
