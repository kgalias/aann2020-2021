{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [Chapter 10](https://www.deeplearningbook.org/contents/rnn.html) of the Deep Learning book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utils for today's class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "from itertools import chain\n",
    "from typing import Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, targets):\n",
    "        \n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, ind):\n",
    "        \n",
    "        return self.data[ind], self.targets[ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    \n",
    "def unicode_to__ascii(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
    "                                                                 and c in all_letters)\n",
    "                   \n",
    "\n",
    "def read_lines(filename: str) -> List[str]:\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to__ascii(line) for line in lines]\n",
    "\n",
    "\n",
    "def letter_to_index(letter: str) -> int:\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "def line_to_tensor(line: str) -> torch.Tensor:\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "The models we've used so far have no concept of memory. This can be a major shortcoming in certain contexts and might be an inductive bias we want to include. This is visible especially in the case of sequential data:\n",
    "* natural language,\n",
    "* time series,\n",
    "* sound.\n",
    "\n",
    "Recurrent neural networks are one way of dealing with the above. They have loops, allowing information to persist.\n",
    "\n",
    "![layer based](figures/unfold.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/rnn.html\">Chapter 10 </a>of the Deep Learning book.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll be working with a simple prediction task (by Sean Robertson). The cell below downloads a dataset of names of 18 different nationalities. Each letter in a name is changed into the so-called *one-hot encoding* and in the end each name is an array of shape `(n_letters, len(name))` consisting of zeroes and ones. \n",
    "\n",
    "We'll also use a sampler because the dataset is quite imbalanced and we want the network to see the same amount of examples from each class.\n",
    "\n",
    "Because the names can have different lengths we'll use `batch_size=1` in this notebook (though the model implementations should work for any values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-24 16:05:18--  https://download.pytorch.org/tutorial/data.zip\n",
      "Resolving download.pytorch.org (download.pytorch.org)... 2600:9000:20ae:7a00:d:607e:4540:93a1, 2600:9000:20ae:1000:d:607e:4540:93a1, 2600:9000:20ae:f200:d:607e:4540:93a1, ...\n",
      "Connecting to download.pytorch.org (download.pytorch.org)|2600:9000:20ae:7a00:d:607e:4540:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2882130 (2,7M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]   2,75M  3,87MB/s    in 0,7s    \n",
      "\n",
      "2021-05-24 16:05:19 (3,87 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/names'\n",
    "\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "data = []\n",
    "targets = [] \n",
    "label_to_idx = {}\n",
    "\n",
    "for label, file_name in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    label_to_idx[label] = file_name.split('.')[0].lower()\n",
    "    \n",
    "    names = read_lines(os.path.join(data_dir, file_name))\n",
    "    data += [line_to_tensor(name) for name in names]\n",
    "    targets += len(names) * [label]\n",
    "\n",
    "test_frac = 0.1\n",
    "n_test = int(test_frac * len(targets))\n",
    "test_ind = np.random.choice(len(targets), size=n_test, replace=False)\n",
    "train_ind = np.setdiff1d(np.arange(len(targets)), test_ind)\n",
    "\n",
    "targets = torch.tensor(targets)\n",
    "train_targets = targets[train_ind]\n",
    "\n",
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = len(targets) / counts\n",
    "weight = [weight_per_class[c] for c in train_targets]\n",
    "\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) \n",
    "\n",
    "train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n",
    "\n",
    "test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows an example datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 3, 52])\n",
      "name: Gil\n",
      "y: korean\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"name: \", end=\"\")\n",
    "for letter_onehot in x[0]:\n",
    "    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n",
    "\n",
    "print(\"\\ny:\", label_to_idx[y.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (0.5p)\n",
    "Implement a basic recurrent neural network.\n",
    "\n",
    "![layer based](figures/LSTM3-SimpleRNN.png)\n",
    "<center>Source: <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>.</center>\n",
    "\n",
    "Some remarks:\n",
    "* Define the needed layers and implement the basic logic for one timestep in the `RNN` class.\n",
    "* The output can be of any submitted size, hence you'll need to add a fully-connected layer at the end.\n",
    "* You'll need to iterate over the time dimension in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size: int,\n",
    "                 hidden_size: int, \n",
    "                 output_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        :param output_size: int\n",
    "            Desired dimensionality of the output vector\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_hidden = torch.nn.Linear(in_features=input_size+hidden_size, out_features=hidden_size)  \n",
    "        self.hidden_to_output = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n",
    "    \n",
    "    # for the sake of simplicity the forward pass will process a single timestep \n",
    "    def forward(self, \n",
    "                input: torch.tensor, \n",
    "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        :param input: torch.tensor \n",
    "            Input tensor for a single observation at a given timestep\n",
    "            shape [batch_size, input_size]\n",
    "        :param hidden: torch.tensor\n",
    "            Representation of the memory of the RNN from the previous timestep\n",
    "            shape [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "\n",
    "        combined = torch.cat([input, hidden], dim=1) \n",
    "        hidden = torch.tanh(self.input_to_hidden(combined))\n",
    "        output = self.hidden_to_output(hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the initial value for the hidden state\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kg/miniconda3/envs/aann/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Progress: 100% Loss: 1.658\n",
      "Final F1 score: 0.20\n"
     ]
    }
   ],
   "source": [
    "n_class = len(label_to_idx)\n",
    "\n",
    "rnn = RNN(n_letters, 256, n_class)\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    loss_buffer = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get initial hidden state\n",
    "        hidden = rnn.init_hidden(x.shape[0])\n",
    "        \n",
    "        # iterate over the time dimension\n",
    "        seq_len = x.shape[1]      \n",
    "        for t in range(seq_len):\n",
    "            # define output here\n",
    "            x_t = x[:, t]\n",
    "            output, hidden = rnn(input=x_t, hidden=hidden)\n",
    "            \n",
    "        loss = cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        loss_buffer.append(loss.item())\n",
    "        \n",
    "        if i % 1000 == 1:\n",
    "            print(f\"\\rEpoch: {epoch+1} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\", end=\"\")\n",
    "            loss_buffer = []\n",
    "    \n",
    "\n",
    "# evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    ps = []\n",
    "    ys = []\n",
    "    correct = 0\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        ys.append(y.numpy())\n",
    "\n",
    "        hidden = rnn.init_hidden(x.shape[0])\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        for t in range(seq_len):\n",
    "            # define output here\n",
    "            x_t = x[:, t]\n",
    "            output, hidden = rnn(input=x_t, hidden=hidden)\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        ps.append(pred.numpy())\n",
    "    \n",
    "    ps = np.concatenate(ps, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    f1 = f1_score(ys, ps, average='weighted')\n",
    "    \n",
    "    print(f\"\\nFinal F1 score: {f1:.2f}\")\n",
    "    assert f1 > 0.15, \"You should get f1 score over 0.15\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (0.25p)\n",
    "Implement the `predict` function, which takes in a name and a RNN model and outputs the top 3 nationality predictions for that name, as well as their logits.\n",
    "\n",
    "Hint: use one of the functions from the beginning of the notebook and [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name: str, rnn: RNN):\n",
    "    x = line_to_tensor(name).unsqueeze(0)\n",
    "\n",
    "    hidden = rnn.init_hidden(x.shape[0])\n",
    "    seq_len = x.shape[1]\n",
    "    for t in range(seq_len): \n",
    "        x_t = x[:, t]\n",
    "        output, hidden = rnn(input=x_t, hidden=hidden)\n",
    "\n",
    "    res = output.topk(3, dim=1)\n",
    " \n",
    "    for score, ind in zip(res.values[0], res.indices[0]):\n",
    "        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satoshi\n",
      "\tjapanese: 4.42\n",
      "\tarabic: 2.73\n",
      "\tpolish: 2.66\n",
      "Jackson\n",
      "\tscottish: 3.60\n",
      "\tdutch: 2.33\n",
      "\tenglish: 2.31\n",
      "Schmidhuber\n",
      "\tgerman: 3.05\n",
      "\tarabic: 2.97\n",
      "\tdutch: 2.58\n",
      "Hinton\n",
      "\tscottish: 2.95\n",
      "\trussian: 2.09\n",
      "\tpolish: 2.06\n",
      "Kowalski\n",
      "\tpolish: 6.51\n",
      "\trussian: 2.99\n",
      "\tjapanese: 2.76\n"
     ]
    }
   ],
   "source": [
    "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
    "\n",
    "for name in some_names:\n",
    "    print(name)\n",
    "    predict(name, rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (0.75p)\n",
    "Implement the LSTM network.\n",
    "\n",
    "![layer based](figures/LSTM3-chain.png)\n",
    "<center>Source: <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a>.</center>\n",
    "\n",
    "* The `LSTMCell` class should contain the needed layers for the LSTM gates.\n",
    "* The `LSTM` class should use the `LSTMCell` class. The iteration over timesteps that was previously done in the training loop should now be moved here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        \"\"\"   \n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # initialize LSTM weights \n",
    "        self.W_f = torch.nn.Linear(in_features=hidden_size+input_size, out_features=hidden_size)\n",
    "        self.W_i = torch.nn.Linear(in_features=hidden_size+input_size, out_features=hidden_size)\n",
    "        self.W_c = torch.nn.Linear(in_features=hidden_size+input_size, out_features=hidden_size)\n",
    "        self.W_o = torch.nn.Linear(in_features=hidden_size+input_size, out_features=hidden_size)\n",
    "\n",
    "    def forward(self, \n",
    "                input: torch.tensor, \n",
    "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \n",
    "        hidden, cell = states  \n",
    "        combined = torch.cat([input, hidden], dim=1)\n",
    "        \n",
    "        # compute input, forget, and output gates       \n",
    "        forget_gate = torch.sigmoid(self.W_f(combined))\n",
    "        input_gate = torch.sigmoid(self.W_i(combined))\n",
    "        output = torch.sigmoid(self.W_o(combined))\n",
    "\n",
    "        # compute new cell state and hidden state\n",
    "        cell_update = torch.tanh(self.W_c(combined))\n",
    "        cell = forget_gate * cell + input_gate * cell_update  \n",
    "        hidden = output * torch.tanh(cell)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size: int, \n",
    "                 hidden_size: int):\n",
    "        \"\"\"\n",
    "        :param input_size: int\n",
    "            Dimensionality of the input vector\n",
    "        :param hidden_size: int\n",
    "            Dimensionality of the hidden space\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.cell = LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "        \n",
    "    def forward(self, \n",
    "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        :param input: torch.tensor\n",
    "            Input tensor for a single observation \n",
    "            shape [batch_size, seq_len, input_size]\n",
    "        Returns tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        hidden, cell = self.init_hidden_cell(batch_size)\n",
    "                \n",
    "        hiddens = []\n",
    "        cells = []\n",
    "        \n",
    "        # process the whole sequence in the forward method (as opposed to the previous exercise)        \n",
    "        seq_len = input.shape[1]\n",
    "        for t in range(seq_len):\n",
    "            x = input[:, t]\n",
    "            hidden, cell = self.cell(x, (hidden, cell))\n",
    "            \n",
    "            hiddens.append(hidden)\n",
    "            cells.append(cell)\n",
    "\n",
    "        return torch.stack(hiddens), torch.stack(cells)\n",
    "    \n",
    "    def init_hidden_cell(self, batch_size) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"\n",
    "        Returns the initial value for the hidden and cell states\n",
    "        \"\"\"\n",
    "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True), \n",
    "                torch.zeros(batch_size, self.hidden_size, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Progress: 100% Loss: 0.910\n",
      "Final F1 score: 0.21\n"
     ]
    }
   ],
   "source": [
    "# initialize the LSTM network with an additional classifier layer on top\n",
    "lstm = LSTM(input_size=len(all_letters), hidden_size=128)\n",
    "clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx))\n",
    "\n",
    "params = chain(lstm.parameters(), clf.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.01) \n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    loss_buffer = []\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden, _ = lstm(x)\n",
    "        output = clf(hidden[-1])\n",
    "\n",
    "        loss = cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()                                \n",
    "        \n",
    "        loss_buffer.append(loss.item())\n",
    "        \n",
    "        if i % 1000 == 1:\n",
    "            print(f\"\\rEpoch: {epoch+1} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\", end=\"\")\n",
    "            loss_buffer = []\n",
    "\n",
    "# evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ps = []\n",
    "    ys = []\n",
    "    for i, (x, y) in enumerate(test_loader): \n",
    "        \n",
    "        ys.append(y.numpy())\n",
    "        \n",
    "        hidden, _ = lstm(x)\n",
    "        output = clf(hidden[-1])\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        ps.append(pred.numpy())\n",
    "    \n",
    "    ps = np.concatenate(ps, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    f1 = f1_score(ys, ps, average='weighted')\n",
    "    \n",
    "    print(f\"\\nFinal F1 score: {f1:.2f}\")\n",
    "    assert f1 > 0.18, \"You should get f1 score over 0.18\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (0.25p)\n",
    "Implement the `predict` function for the LSTM+CLF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n",
    "    x = line_to_tensor(name).unsqueeze(0)\n",
    "\n",
    "    hidden, _ = lstm(input=x)\n",
    "    output = clf(hidden[-1])\n",
    "\n",
    "    res = output.topk(3, dim=1)\n",
    " \n",
    "    for score, ind in zip(res.values[0], res.indices[0]):\n",
    "        print(f\"\\t{label_to_idx[ind.item()]}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satoshi\n",
      "\tjapanese: 3.76\n",
      "\titalian: 0.33\n",
      "\tgreek: -0.47\n",
      "Jackson\n",
      "\tscottish: 5.12\n",
      "\tenglish: 2.06\n",
      "\tdutch: -0.83\n",
      "Schmidhuber\n",
      "\tgerman: 4.48\n",
      "\tczech: 3.35\n",
      "\trussian: 2.93\n",
      "Hinton\n",
      "\tscottish: 2.30\n",
      "\tenglish: 1.83\n",
      "\tdutch: -0.66\n",
      "Kowalski\n",
      "\tpolish: 4.98\n",
      "\tjapanese: 1.19\n",
      "\trussian: 0.51\n"
     ]
    }
   ],
   "source": [
    "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
    "    \n",
    "for name in some_names:\n",
    "    print(name)\n",
    "    predict_lstm(name, lstm, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
