{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was inspired by neural network & machine learning labs led by [GMUM](https://gmum.net/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks\n",
    "\n",
    "There are three necessary ingredients to train a neural network:\n",
    "\n",
    "* the model,\n",
    "* the loss,\n",
    "* the optimizer.\n",
    "\n",
    "We've already implemented a popular loss function in the first lab. Today we will briefly remind ourselves what the most popular optimizer (SGD) does and implement a custom model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "As a brief recap, *stochastic gradient descent* is an iterative method for optimizing an *objective function* (or *criterion*; when minimizing also *cost function*, *loss function*, or *error function*) which we can calculate the gradients of.\n",
    "\n",
    "One way of minimizing the cost function $L(X; \\theta)$ for a set of data $X \\in \\mathbb{R}^{NxD}$ is calculating the average cost of all the elements $\\mathbf{x} \\in X$:\n",
    "\n",
    "$$L(X; \\theta) = \\frac{1}{N} \\sum_i L(\\mathbf{x}_i; \\theta).$$\n",
    "\n",
    "Next one could calculate the gradient of this and use that to minimize the function:\n",
    "\n",
    "$$\\theta_{new}=\\theta_{old} -\\alpha \\nabla_\\theta L(X; \\theta),$$\n",
    "\n",
    "with $\\alpha$ being the step size. We would then apply this iteratively until convergence.\n",
    "\n",
    "![gradient descent](figures/fig4.png)\n",
    "<center>Source: <a href=\"https://www.deeplearningbook.org/contents/numerical.html\">Chapter 4</a> of the Deep Learning book.</center>\n",
    "\n",
    "In practice, our dataset could turn out to be enormous. It would be impractical to calculate the loss (and the gradient) for the whole dataset. We usually replace that with the cost function and gradient over a subset of $X$, a so-called *batch* $B \\subsetneq X$:\n",
    "\n",
    "$$L(B; \\theta) = \\frac{1}{|B|} \\sum_{\\mathbf{x} \\in B} L(\\mathbf{x}; \\theta).$$\n",
    "\n",
    "Doing SGD instead of GD also has good consequences for generalization, which we might talk about in the future.\n",
    "\n",
    "The gradient of the cost calculated on the batch is probably going to be different than the gradient calculated on the whole dataset, but we can use it as an approximation, trading off iteration time against convergence rate:\n",
    "\n",
    "\n",
    "$$\\nabla_\\theta L(B; \\theta) \\approx \\nabla_\\theta L(X; \\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an actual network for classification on the FashionMNIST dataset in PyTorch. First, however, we need to prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (0.5p)\n",
    "Prepare the data which we'll be using for the next task.\n",
    "You need to, using [transforms](https://pytorch.org/vision/0.8/transforms.html) (following first week's lab):\n",
    "- convert the PIL images to tensors,\n",
    "- calculate the mean and standard deviation of pixels of the training set and use that to normalize the training data (this is new),\n",
    "- change the shape of each image from 28x28 to 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ???\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_and_std() -> Tuple[float, float]:\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        FashionMNIST(\n",
    "            root='.',\n",
    "            download=True,\n",
    "            train=True,\n",
    "            transform=ToTensor()\n",
    "        )\n",
    "    )\n",
    "    ???\n",
    "    return mean, std\n",
    "\n",
    "mean, std = calculate_mean_and_std()\n",
    "\n",
    "train_data = FashionMNIST(root='.', \n",
    "                          download=True, \n",
    "                          train=True, \n",
    "                          ???)\n",
    "\n",
    "test_data = FashionMNIST(root='.', \n",
    "                         download=True, \n",
    "                         train=False, \n",
    "                         ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks whether the mean and std calculation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(mean, 0.286, atol=1e-4)\n",
    "assert np.isclose(std, 0.353, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks whether the dataloader returns objects of appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=10)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "assert len(x.shape) == 2\n",
    "assert x.shape == (10, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to building our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1p)\n",
    "Implement a simple neural network in Pytorch. \n",
    "\n",
    "The network is supposed to accept data of dimension `input_dim` and have one hidden layer of size `hidden_dim` with weights initialized from the standard normal distribution. The biases are supposed to be initialized with zeros. For the activation function for the first layer use `torch.tanh`. For the second layer use a linear activation function. Don't forget to use `requires_grad=True` when defining the parameters of the network.\n",
    "\n",
    "Next, implement a training loop in PyTorch utilizing the cost function `nn.CrossEntropyLoss` and the SGD optimizer.\n",
    "\n",
    "If everything was implemented correctly, the network should usually achieve accuracy higher than $80\\%$ on the test set (you might need a few runs for this, depending on the initialization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class CustomNetwork(object):\n",
    "    \"\"\"\n",
    "    Simple 1-hidden-layer linear neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the network's weights \n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight_1: torch.Tensor = ???\n",
    "        self.bias_1: torch.Tensor = ???\n",
    "        \n",
    "        self.weight_2: torch.Tensor = ???\n",
    "        self.bias_2: torch.Tensor = ???\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        \"\"\"\n",
    "        ???\n",
    "        \n",
    "    def parameters(self) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a list of all trainable parameters \n",
    "        \"\"\"\n",
    "        return [self.weight_1, self.bias_1, self.weight_2, self.bias_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks whether the network's weights have appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CustomNetwork(100, 32, 54)\n",
    "\n",
    "assert network.weight_1.shape == (100, 32)\n",
    "assert network.weight_2.shape == (32, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "# some hyperparams\n",
    "batch_size: int = 64\n",
    "n_epochs: int = 10\n",
    "\n",
    "# prepare data loaders based on the already loaded datasets\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# initialize the model\n",
    "model: CustomNetwork = ???\n",
    "\n",
    "# initialize the optimizer using the hyperparams below\n",
    "lr: float = 0.01\n",
    "momentum: float = 0.9\n",
    "optimizer: torch.optim.Optimizer = SGD(???, lr=lr, momentum=momentum)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "for e in range(n_epochs):\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        # reset the gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # pass through the network\n",
    "        output: torch.Tensor = ???\n",
    "        # calculate loss\n",
    "        loss: torch.Tensor = criterion(???)\n",
    "        # backward pass thorught the network\n",
    "        loss.backward()\n",
    "        # apply the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log the loss value\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"\\rEpoch {e+1} iter {i+1}/{len(train_data) // batch_size} loss: {loss.item()}\", end=\"\")\n",
    "            \n",
    "    # at the end of an epoch run evaluation on the test set\n",
    "    with torch.no_grad():\n",
    "        # initialize the number of correct predictions\n",
    "        correct: int = 0 \n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # pass through the network\n",
    "            output: torch.Tensor = ???\n",
    "            correct += ???\n",
    "\n",
    "        print(f\"\\nTest accuracy: {correct / len(test_data)}\")\n",
    "\n",
    "        \n",
    "# this is your test\n",
    "assert correct / len(test_data) > 0.8, \"Subject to random seed you should be able to get >80% accuracy\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
